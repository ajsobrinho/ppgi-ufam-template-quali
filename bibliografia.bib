@article{Ribeiro2018,
	abstract = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, "sufficient" conditions for predictions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
	annote = {Mesmo autor do LIME
	
	pra detectar imagens poderia ter usado meaningful parturbations!
	
	breed?},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	file = {:C$\backslash$:/Users/Ant{\^{o}}nio J{\'{u}}nior/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ribeiro, Singh, Guestrin - 2018 - Anchors High-Precision Model-Agnostic Explanations.pdf:pdf},
	journal = {Thirty-Second AAAI Conference on Artificial Intelligence},
	keywords = {interpretability,machine learning},
	mendeley-groups = {Revis{\~{a}}o 1/1. MANUAL,RefQualifica{\c{c}}{\~{a}}o},
	month = {apr},
	title = {{Anchors: High-Precision Model-Agnostic Explanations}},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16982},
	year = {2018}
}
@inproceedings{Ribeiro:2016:WIT:2939672.2939778,
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	address = {New York, NY, USA},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	booktitle = {Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	doi = {10.1145/2939672.2939778},
	file = {:C$\backslash$:/Users/Ant{\^{o}}nio J{\'{u}}nior/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ribeiro, Singh, Guestrin - 2016 - Why Should I Trust You Explaining the Predictions of Any Classifier.pdf:pdf},
	isbn = {978-1-4503-4232-2},
	keywords = {black box classifier,explaining machine learning,interpretability,interpretable machine learning},
	mendeley-groups = {Revis{\~{a}}o 1/2. ACM,RefQualifica{\c{c}}{\~{a}}o},
	pages = {1135--1144},
	publisher = {ACM},
	series = {KDD '16},
	title = {{"Why Should I Trust You?": Explaining the Predictions of Any Classifier}},
	url = {http://doi.acm.org/10.1145/2939672.2939778},
	year = {2016}
}

@article{Guidotti2018,
	annote = {Se{\c{c}}{\~{o}}es mais importantes: 
	5, 
	6, 7 8 e 9
	10},
	author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
	doi = {10.1145/3236009},
	file = {:C$\backslash$:/Users/Ant{\^{o}}nio J{\'{u}}nior/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guidotti et al. - 2018 - A Survey of Methods for Explaining Black Box Models.pdf:pdf},
	issn = {03600300},
	journal = {ACM Computing Surveys},
	keywords = {Open the black box,explanations,interpretability,transparent models},
	mendeley-groups = {Revis{\~{a}}o 1/1. MANUAL},
	month = {aug},
	number = {5},
	pages = {1--42},
	publisher = {ACM},
	title = {{A Survey of Methods for Explaining Black Box Models}},
	url = {http://dl.acm.org/citation.cfm?doid=3271482.3236009},
	volume = {51},
	year = {2018}
}
